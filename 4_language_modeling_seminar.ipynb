{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Language_modeling_seminar.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kwqhVfKZFTsH"},"source":["<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" style=\"height:450px;\" width=500/></p>\n","\n","<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n","<h3 style=\"text-align: center;\"><b>Продвинутый поток (часть 2). Весна 2021</b></h3>\n","\n","<h1 style=\"text-align: center;\"><b>Language modeling.</b></h1>"]},{"cell_type":"markdown","metadata":{"id":"yoqejrGuDtNT"},"source":["## Installation and Dataset"]},{"cell_type":"markdown","metadata":{"id":"4qfMzq3yGMfz"},"source":["Для начала загрузим датасет, состоящий из сэмплов кода на языке Python. Датасет представлен гитхабом. [Про датасет](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/).\n","\n","Для препроцессинга будем использовать уже известную нам библиотеку `datasets` от Huggingface."]},{"cell_type":"code","metadata":{"id":"cRr0qansUVKx","executionInfo":{"status":"ok","timestamp":1617015071897,"user_tz":-60,"elapsed":4867,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["!pip install -q datasets"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kNmgObmKuEi","executionInfo":{"status":"ok","timestamp":1617015130497,"user_tz":-60,"elapsed":63463,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"2bd08392-5fc7-44b8-9c09-2f749fc0ba54"},"source":["!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n","!unzip -p python.zip python/final/jsonl/train/python_train_0.jsonl.gz > train.jsonl.gz\n","!unzip -p python.zip python/final/jsonl/test/python_test_0.jsonl.gz > test.jsonl.gz"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-03-29 10:51:11--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.108.125\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.108.125|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 940909997 (897M) [application/zip]\n","Saving to: ‘python.zip.1’\n","\n","python.zip.1        100%[===================>] 897.32M  16.3MB/s    in 57s     \n","\n","2021-03-29 10:52:09 (15.6 MB/s) - ‘python.zip.1’ saved [940909997/940909997]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1T5RyVptLCYU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9ac6831-8573-40d6-a1d8-052bca08637a"},"source":["# decompress this gzip file\n","!gzip -d train.jsonl.gz\n","!gzip -d test.jsonl.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gzip: train.jsonl already exists; do you wish to overwrite (y or n)? "],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_HOfyBhlImmr"},"source":["Загружать датасеты можно не только из хаба, но и из диска. Для этого достаточно указать формат и путь до файла.\n","\n","Datasets can be downloaded not only from the Hub, but also from Drive. To do this, specify file's format and path."]},{"cell_type":"code","metadata":{"id":"QWVVewKKUNXu"},"source":["from datasets import load_dataset  \n","dataset = load_dataset(\n","    \"json\",\n","    data_files=[\n","        \"train.jsonl\",\n","    ],\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3x7ZFbnM5vR"},"source":["dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3P6WOO6Iw-L"},"source":["Ограничим число уникальных слов до `40000`.\n","\n","Bound the number of uniqie words by `40000`."]},{"cell_type":"code","metadata":{"id":"1DfCJVx7PmiX"},"source":["import tqdm\n","from collections import Counter\n","\n","\n","vocab_size = 40000\n","stats = Counter()\n","\n","# stats is like word2freq dictionary\n","for item in tqdm.tqdm(dataset[\"train\"]):\n","    stats.update(item[\"code_tokens\"])\n","\n","# select vocab_size most common words from stats\n","#   and extract the keys, they will be our vokabulary and consequently - our tokens\n","tokens = dict(stats.most_common(vocab_size)).keys()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmkeF_DEJDLv"},"source":["Have a look at 20 most frequent words. These were expected becouuse the data contains python code."]},{"cell_type":"code","metadata":{"id":"zK81AWTmSDoh"},"source":["stats.most_common(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeYOYOdcJIAQ"},"source":["1. Add *service* tokens \"[PAD]\", \"[UNK]\", \"[EOS]\" (end of sentense).\n","\n","2. Make `token2idx` dictionary (just enumerate them all)\n","\n","3. Make`idx2tiken` - inverse of `token2idx`"]},{"cell_type":"code","metadata":{"id":"1bI5Z3K_R6ZO"},"source":["# service tokens\n","PAD = 0\n","UNK = 1\n","EOS = 2\n","token2idx = {\"[PAD]\": PAD, \"[UNK]\": UNK, \"[EOS]\": EOS}\n","\n","# token2idx\n","for idx, token in enumerate(tokens):\n","    token2idx[token] = idx + 3\n","\n","# token2idx\n","idx2token = {idx: token for token, idx in token2idx.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lCB9ztKvGHLr"},"source":["Let's make a function which encodes the tokens as indexes."]},{"cell_type":"code","metadata":{"id":"MKWCEMGLjosj"},"source":["def encode(token):\n","    \"\"\"\n","    returns\n","        for known tokens - their indexes\n","        for unknown tokens - index of the '[UNK]' token\n","    \"\"\"\n","    if token in token2idx.keys():\n","        return token2idx[token]\n","    return UNK"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Uz228mRJOHK"},"source":["Encode tokens as indexes in the train dataset."]},{"cell_type":"code","metadata":{"id":"GPLkFE44O-rD"},"source":["dataset = dataset.map(\n","    lambda item: {\n","        \"features\": [encode(token) for token in item[\"code_tokens\"]] + [EOS]\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lvklCYVfKSp8"},"source":["## N-gram\n","\n"," Наченм с простейшей модели. Она основывается на статистическом методе. Итак, в языковом моделировании мы хотим максимизировать вероятность нашего текста по мнению модели, то есть:\n"," $$\n","\\mathrm{P}(\\mathrm{W})=\\mathrm{P}\\left(\\mathrm{w}_{1}, \\mathrm{w}_{2}, \\mathrm{w}_{3}, \\mathrm{w}_{4}, \\mathrm{w}_{5} \\ldots \\mathrm{w}_{\\mathrm{n}}\\right)\n","$$\n","\n","\n","Вспомним, что можно переписать:\n","\n","$$\n","P\\left(x_{1}, x_{2}, x_{3}, \\ldots, x_{n}\\right)=P\\left(x_{1}\\right) P\\left(x_{2} \\mid x_{1}\\right) P\\left(x_{3} \\mid x_{1}, x_{2}\\right) \\ldots P\\left(x_{n} \\mid x_{1}, \\ldots, x_{n-1}\\right)\n","$$\n","\n","Тогда:\n","\n","$$\n","P\\left(w_{1} w_{2} \\ldots w_{n}\\right)=\\prod_{i} P\\left(w_{i} \\mid w_{1} w_{2} \\ldots w_{i-1}\\right)\n","$$\n","\n","Однако число вероятностей вида $P\\left(w_{i} \\mid w_{1} w_{2} \\ldots w_{i-1}\\right)$ растет очень быстро. Поэтому используют некоторое предположение которое называется **марковковское приближение**. Формулируется оно так:\n","\n","$$\n","P\\left(w_{1} w_{2} \\ldots w_{n}\\right) \\approx \\prod_{i} P\\left(w_{i} \\mid w_{i-k} \\ldots w_{i-1}\\right)\n","$$\n","\n","То есть мы считаем, что текущее слово зависит только от $k$ предыдущих.\n","\n","$$\n","P\\left(w_{i} \\mid w_{1} w_{2} \\ldots w_{i-1}\\right) \\approx P\\left(w_{i} \\mid w_{i-k} \\ldots w_{i-1}\\right)\n","$$\n"]},{"cell_type":"code","metadata":{"id":"iTEcwLLWh7R7"},"source":["import numpy as np\n","from collections import Counter, defaultdict\n","\n","from tqdm.notebook import tqdm\n","\n","\n","class NGramModel(object):\n","    \"\"\"\n","    Структура этой реализации n-граммной модели следующая:\n","    self.ngrams – словарь, который на каждый (token_0, ..., token_(n-1)) – n-1 tuple из токенов\n","        хранит частоту появления следующего токена. Для подсчета числа токенов воспользуемся\n","        Counter\n","    self.tokenize_func – функция токенизации текста. С её помощью будем получать токены.\n","    \"\"\"\n","    def __init__(self, n=2):\n","        self.n = n\n","        self.tokenize_func = None\n","        # maps tuples of tokens to their freqs\n","        self.ngrams = defaultdict(Counter)\n","        \n","    def compute_ngrams(self, dataset):\n","        self.ngrams = defaultdict(Counter)\n","        for row in tqdm(dataset):\n","            ngram = [PAD] * self.n\n","            for token in row[\"features\"]:\n","                # shift the window towards the end of the sentence and add new token\n","                ngram[:-1] = ngram[1:]\n","                ngram[-1] = token\n","                self.ngrams[tuple(ngram[:-1])].update([ngram[-1]])\n","            \n","    def get_log_probs(self, prefix, min_log_pr=-15):\n","        \"\"\"\n","        returns log frequences of token occurrences\n","        \"\"\"\n","        # small prefix => need to pad at the beginning\n","        if len(prefix) < self.n - 1:\n","            prefix = [PAD] * (self.n - len(prefix) - 1) + prefix\n","        # big prefix => just take the relevant tail\n","        else:\n","            prefix = prefix[-self.n + 1:]\n","\n","        possible_ends = self.ngrams[tuple(prefix)]\n","        sum_freq = np.log(sum(possible_ends[e] for e in possible_ends))\n","\n","        # log(a/b) = log(a) - log(b); RHS is much more stable\n","        return {e: np.log(possible_ends[e]) - sum_freq for e in possible_ends}\n","    \n","    def sample(self, prefix):\n","        possible_ends = self.get_log_probs(prefix)\n","        if len(possible_ends) > 0:\n","            end = np.random.choice(list(possible_ends.keys()), p=np.exp(list(possible_ends.values())))\n","            return end\n","        return EOS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dyat2FIEKE_t"},"source":["### Training\n","\n","Initialize the model."]},{"cell_type":"code","metadata":{"id":"AZztYXDZi1Aj"},"source":["n_gram_model = NGramModel(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HbTn-5jPKFvs"},"source":["Train the model."]},{"cell_type":"code","metadata":{"id":"OQ5aJbIOjVhZ"},"source":["n_gram_model.compute_ngrams(dataset[\"train\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wK9PODHBKNO4"},"source":["### Text generation\n","\n","Generate some code with the n-gram model."]},{"cell_type":"code","metadata":{"id":"aQfO1RV7jaXj"},"source":["prefix = [\"def\", \"train\", \"(\"]\n","encoded_prefix = [token2idx[token] for token in prefix]\n","length=100\n","\n","for i in range(length):\n","    cur_token = n_gram_model.sample(encoded_prefix)\n","    if cur_token == EOS:\n","        break\n","    encoded_prefix += [cur_token]\n","\n","\n","decoded_text = [idx2token[idx] for idx in encoded_prefix]\n","print(\" \".join(decoded_text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ueVeYXRsKUnB"},"source":["### Testing"]},{"cell_type":"code","metadata":{"id":"t7qmBK6Tpi5B"},"source":["test_dataset = load_dataset(\n","    \"json\",\n","    data_files=[\n","        \"test.jsonl\",\n","    ],\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Txq5x7dUqEM-"},"source":["max_seq_len=128\n","\n","test_dataset = test_dataset.map(\n","    lambda item: {\n","        \"features\": [encode(token) for token in item[\"code_tokens\"]][:max_seq_len-1] + [EOS]\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkTOYEwtwTGZ"},"source":["### Evaluation metric: Perplexity (PP)\n","\n","See [Perplexity in Language Models](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94): \n","\n","*Intuitively, if a model assigns a high probability to the test set, it means that it is not surprised to see it (it’s **not perplexed** by it), which means that it has a good understanding of how the language works.*\n","\n","$$\n","P P(p):=2^{H(p)}=2^{-\\sum_{x} p(x) \\log _{2} p(x)}\n","$$\n","\n","From what we know of cross-entropy we can say that $H(W)$ is the **average number of bits needed to encode each word**. This means that the perplexity $2^{H(W)}$ is the **average number of words that can be encoded using $H(W)$ bits.**\n","\n","\n","**We can also use**\n","\n","$$\n","P P' (p):=e^{H(p)}=e^{-\\sum_{x} p(x) \\ln p(x)}\n","$$\n","\n","And the averaged over the number of samples version.\n","\n","$$\n","P P'' (p):=e^{H(p)}=e^{-\\frac{1}{n}\\sum_{x} p(x) \\ln p(x)}\n","$$"]},{"cell_type":"code","metadata":{"id":"4yZWod2FunaZ"},"source":["def count_perplexity(model, dataset, max_iter_num: int = 1000):\n","    \"\"\"\n","    \n","    \"\"\"\n","    entropy = 0\n","    iter_num = 0\n","    num_words = 0\n","    for item in tqdm(dataset, total=min(max_iter_num, len(dataset))):\n","        output_so_far = [item[\"features\"][0]]\n","\n","        for token in item[\"features\"][1:]:\n","            num_words += 1\n","            try:\n","                log_probs = model.get_log_probs(output_so_far)\n","                entropy += -log_probs[token] # for all other words in the vocab \n","            except KeyError:\n","                entropy += np.log(-10)\n","            output_so_far.append(token)\n","        iter_num += 1\n","        if iter_num > max_iter_num:\n","            break\n","    mean_entropy = entropy / num_words\n","    return np.e ** mean_entropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W71ZBELF2V_L"},"source":["count_perplexity(n_gram_model, test_dataset[\"train\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QbCr0x3mSc-"},"source":["## CNN\n","\n","![](https://lena-voita.github.io/resources/lectures/lang_models/neural/cnn/cnn_main-min.png)\n","\n"]},{"cell_type":"code","metadata":{"id":"QkTpNqkW2evJ"},"source":["# transform lists into torch tensors (still of varying lengths)\n","dataset.set_format(type=\"torch\", columns=[\"features\"])\n","test_dataset.set_format(type=\"torch\", columns=[\"features\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhnKfYs95WFQ"},"source":["def collate_fn(batch):\n","    \"\"\"\n","    1. Takes a batch as an argument\n","    2. Extracts a list of tensors (featuers) of varying lengths as a batch['features']\n","    3. Padds them so all the tensors in a batch have the same lengths and puts them \n","        all in one tensor `input_embeds`\n","\n","    returns: a dictionary {\"features\": input_embeds}\n","    \"\"\"\n","    batch = batch[0]\n","    max_len = max(len(f_t) for f_t in batch[\"features\"])\n","    input_embeds = torch.zeros((len(batch[\"features\"]), max_len), dtype=torch.long)\n","    for idx, row in enumerate(batch[\"features\"]):\n","        input_embeds[idx][:len(row)] += row\n","    return {\n","        \"features\": input_embeds,\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlFF7B_CLw5z"},"source":["This is similar to the Dataset class, but has varying batch size.\n","\n","**Q:** Why do we do this?\n","\n","**A:** Our collate function can result in different number of tokens in the padded sentences in a batch. Sometimes it will be small, sometimes - large. Recall, the batch matrix groes linearly in **T**=`batch_size` $\\times$ `len(longest sentence in the batch)`. If we fix the batch size and encounter a very large sentense in the middle of our training, we might run out of memory and the training will just crash. To avoid that, we will record **T** and stop adding sentences to the batch if we acceed a certaion threshold if we include this one extra sentence. Off course, the amount of memory taken also linearly depends on the size of our `token embedding`.\n"]},{"cell_type":"code","metadata":{"id":"iuFWoWexj2Xj"},"source":["from torch.utils.data import Sampler\n","\n","\n","\n","class TextSampler(Sampler):\n","    def __init__(self, sampler, batch_size_tokens=1e4):\n","        self.sampler = sampler\n","        self.batch_size_tokens = batch_size_tokens\n","\n","    def __iter__(self):\n","        batch = []\n","        max_len = 0\n","        for ix in self.sampler:\n","            row = self.sampler.data_source[ix]\n","            max_len = max(max_len, len(row[\"features\"]))\n","            # if we acceed the number of tokens given as a threshold, we yield batch\n","            #   this means that we will not add the last consedered sentense to the \n","            #   current batch\n","            if (len(batch) + 1) * max_len > self.batch_size_tokens:\n","                yield batch\n","                # after yielding the batch\n","                # this sentense will be recorded as the first one in the fresh batch\n","                batch = []\n","                max_len = len(row[\"features\"])\n","            # in both cases (current batch is old batch or current batch is a fresh batch)\n","            #   we will append the sentence index to the current batch\n","            batch.append(ix)\n","\n","        # if we ran out of sentences and have not yielded the last non-empty batch,\n","        #   it's time to do so now\n","        if len(batch) > 0:\n","            yield batch\n","\n","    def __len__(self):\n","        return len(self.sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKfRZ8gq6Hm-"},"source":["from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, random_split\n","\n","# sample indexes at random every time for better training\n","train_sampler = RandomSampler(dataset[\"train\"])\n","# more efficient sequential sempler works best when there is no need to train (validation/testing)\n","valid_sampler = SequentialSampler(test_dataset[\"train\"])\n","\n","\n","\n","loaders = {\n","    \"train\": DataLoader(\n","        dataset[\"train\"],                           # dataset to use\n","        collate_fn=collate_fn,                      # convert list of inputs into batches\n","        sampler=TextSampler(sampler=train_sampler,) # Sample batches of varying batch size\n","    ),\n","    \"valid\": DataLoader(\n","        test_dataset[\"train\"],                      # dataset to use\n","        collate_fn=collate_fn,                      # convert list of inputs into batches\n","        sampler=TextSampler(sampler=valid_sampler,) # Sample batches of varying batch size\n","    )\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiIAV6jX6z7m"},"source":["import torch\n","import torch.nn as nn\n","\n","\n","class CNNLM(nn.Module):\n","    def __init__(self, vocab_size, emb_size, hidden_size, num_layers=3, kernel_size: int = 5):\n","        super().__init__()\n","        \n","        self.emb = nn.Embedding(vocab_size, emb_size)\n","        layers = []\n","\n","        # PADDING\n","        # YOUR CODE GOES HERE (DOWN)\n","        for layer_idx in range(num_layers):\n","            layers.append(nn.ZeroPad2d((kernel_size-1, 0, 0, 0)))\n","            if layer_idx == 0:\n","                layers.append(nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size))\n","            else:\n","                layers.append(nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size))\n","        # YOUR CODE GOES HERE (UP)\n","\n","\n","        self.conv_layers = nn.Sequential(*layers)\n","        # for receptive_field, check the picture below (red part shows receptive field)\n","        self.receptive_field = kernel_size + (kernel_size-1)*(num_layers-1)\n","        self.pred = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, input_ids):\n","        #print(input_ids.shape)                 # (batch_size, max_len) max_len - maximum sentence length in the batch w/o padding\n","        embed = self.emb(input_ids)             # (batch_size, max_len, emb_size)\n","        #print(embed.shape)\n","        embed = embed.permute(0, 2, 1)          # (batch_size, emb_size, max_len) want to convolve over embeddings for words\n","        #print(embed.shape)\n","        features = self.conv_layers(embed)      # (batch_size, hidden_size, max_len)\n","        #print(features.shape)\n","        features = features.permute(0, 2, 1)    # (batch_size, max_len, hidden_size)\n","        #print(features.shape)\n","        logits = self.pred(features)            # (batch_size, max_len, vocab_size)\n","        #print(logits.shape)\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdJGxl6ADLt_"},"source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = CNNLM(len(tokens) + 3, 300, 100, num_layers=1).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6ooYStKFcJn"},"source":["from tqdm.notebook import tqdm, trange\n","\n","\n","def train(\n","    num_epochs: int, \n","    model: nn.Module,\n","    train_loader: DataLoader,\n","    valid_loader: DataLoader,\n","    criterion: nn.Module,\n","    optimizer: torch.optim.Optimizer,\n","    max_grad_norm: float = None\n","):\n","    for epoch in trange(num_epochs):\n","        pbar = tqdm(train_loader, leave=False, total=len(train_loader)//20)\n","        pbar.set_description(\"Train epoch\")\n","        model.train()\n","        for batch in pbar:\n","            optimizer.zero_grad()\n","            features = batch[\"features\"].to(device)\n","\n","            # we do not take the last token since we will predict it\n","            predictions = model(features[:, :-1])\n","            loss = criterion(\n","                predictions.reshape(-1, predictions.size(-1)), # (batch_size, vocab_size)\n","                features[:, 1:].reshape(-1)                    # (batch_size, )\n","            )\n","            loss.backward()\n","\n","            # gradient clipping\n","            if max_grad_norm is not None:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","            optimizer.step()\n","        model.eval()\n","        mean_loss = 0\n","        pbar = tqdm(valid_loader, leave=False, total=len(valid_loader)//100)\n","        pbar.set_description(\"Valid epoch\")\n","        num_iter=0\n","        for batch in pbar:\n","            features = batch[\"features\"].to(device)\n","            with torch.no_grad():\n","                predictions = model(features[:, :-1])\n","                loss = criterion(\n","                    predictions.reshape(-1, predictions.size(-1)),\n","                    features[:, 1:].reshape(-1)\n","                )\n","            mean_loss += loss.item()\n","            num_iter += 1\n","        mean_loss /= num_iter\n","        print(f\"Epoch: {epoch}; mean loss: {mean_loss}; perplexity: {np.exp(mean_loss)}\")\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oCRpQnHK_TH"},"source":["train(\n","    num_epochs=1,\n","    model=model, \n","    train_loader=loaders[\"train\"],\n","    valid_loader=loaders[\"valid\"],\n","    criterion=criterion,\n","    optimizer=optimizer,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kr1iE9Vtu-mt"},"source":["![](https://lena-voita.github.io/resources/lectures/lang_models/neural/cnn/receptive_field-min.png)\n","\n","\n","Как увеличить receptive field? \n","\n","Добавить больше слоев.\n","\n","Как обучать?\n","\n","Добавить residual connections.\n","\n","\n","![](https://lena-voita.github.io/resources/lectures/lang_models/neural/cnn/cnn_with_residual-min.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_G9NNcoMZcDb"},"source":["### Visualizing Softmax outputs and using Temperature\n","\n","**Q:** What's the deal with temperature?\n","\n","**A:** It comes as a factor into softmax function both in the numerator and in the denominator and either increases the differences between the largest and the smallest values before softmax (low Temperature), or actually makes the softmax output closer to uniform distribution (high Temperature)."]},{"cell_type":"code","metadata":{"id":"Vlcl6vcX1HAF"},"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from ipywidgets import interactive\n","from IPython import display\n","\n","sns.set(style=\"whitegrid\", font_scale=1.4)\n","\n","sample = np.random.randn(10)\n","def plot_temperature(T: float = 1.0):\n","    plt.figure(figsize=(12, 8))\n","    plt.title(f\"Temperature = {T}\")\n","    probs = np.exp(sample / T) / sum(np.exp(sample / T)) # the only neew bit (scaling softmax)\n","    plt.bar(range(10), probs)\n","    plt.xlabel(\"tokens\")\n","    plt.ylabel(\"probs\")\n","    plt.show()\n","\n","\n","v = interactive(\n","    plot_temperature, T=(0.02, 10)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kn-GXW-V3_Ns"},"source":["display.display(v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTn_Cb2GaJ9P"},"source":["### Generate text"]},{"cell_type":"markdown","metadata":{"id":"1pGhbvslexxz"},"source":["Check how we can extract receptive fild from a model. Some models won't have it.\n","\n","We want to know the receptive field to optimize and gove less tokens to the model as input."]},{"cell_type":"code","metadata":{"id":"do_GqU7seFdz"},"source":["try:\n","    model.receptive_fiel\n","except AttributeError as e:\n","    print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRLkLl4yfDC_"},"source":["Make a function to generate text."]},{"cell_type":"code","metadata":{"id":"vz96lopCPIdx"},"source":["from typing import List\n","from torch.distributions import Categorical\n","\n","@torch.no_grad()\n","def generate(\n","    prefix, model, length: int = 100, receptive_field: int = 5, T: float = 1.\n",") -> List[int]:\n","    prefix = torch.from_numpy(prefix)\n","    prefix = prefix.unsqueeze(0).to(device)\n","    model.eval()\n","    for iter_idx in range(length):\n","        # use the knowledge of the receptive_field to optimize\n","        #   the rest of the tokens are not used anyway\n","        try:\n","            preds = model(prefix[:, -model.receptive_field:])\n","        except AttributeError as e:\n","            print(e)\n","            preds = model(prefix[:, -receptive_field:])\n","        # print(preds.shape)                # (batch_size, max_len, vocab_size)\n","\n","\n","        # only interested in the last token before giving it into softmax\n","        # print(preds[:, -1, :].shape)      # (batch_size, vocab_size)\n","\n","        # scale by Temperature before applying softmax on the last dimention (vocab_size)\n","        probs = torch.softmax(preds[:, -1, :]/T, dim=-1)\n","        # print(probs.shape)                # (batch_size, vocab_size)\n","\n","        # to sample from discrete distribution with known probs\n","        #   use torch.distributions.Categorical with method .sample()\n","        distribution = Categorical(probs)\n","        sampled = distribution.sample()\n","\n","        # if we reached the end of the sentence token - break\n","        if sampled.item() == EOS:\n","            break\n","\n","        # record the last-generated token\n","        prefix = torch.cat((prefix, sampled.unsqueeze(0)), dim=1)\n","    return prefix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iQC9ylLdfH9g"},"source":["Generate text for 5 different temperatures. Note, temperature scales within the **softmax** function, so it makes sence to try the temperatures on the **log-scale** (**softmax will exponentiate it** anyway)."]},{"cell_type":"code","metadata":{"id":"RRnzjA-DfcOx"},"source":["prefix = [\"def\", \"train\", \"(\"]\n","encoded_prefix = np.array([token2idx[t] for t in prefix])\n","\n","for t in np.logspace(0.002, 1, 10):\n","    generated = generate(\n","        encoded_prefix, \n","        model, \n","        receptive_field=model.receptive_field, \n","        length=20,\n","        T=t-1\n","    )\n","    print(f\"Temperature: {t-1}\")\n","    print(\" \".join([idx2token[idx] for idx in generated.cpu().numpy().flatten()]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpA1Zm2L_hx0"},"source":["## LSTM\n","\n","![](https://lena-voita.github.io/resources/lectures/lang_models/neural/rnn/rnn_simple-min.png)"]},{"cell_type":"code","metadata":{"id":"nodBgU58g8Rc"},"source":["class LSTM(nn.Module):\n","    def __init__(self, vocab_size, emb_size, hidden_size):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_size)\n","        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True)\n","        self.pred = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, input_ids):\n","        # print(input_ids.shape)    # (batch_size, max_len) max_len - maximum sentence length in the batch w/o padding\n","        embs = self.emb(input_ids)\n","        # print(embs.shape)         # (batch_size, max_len, emb_size)\n","        output, _ = self.lstm(embs)\n","        # print(output.shape)       # (batch_size, max_len, hidden_size)\n","        output = self.pred(output)\n","        #print(output.shape)         # (batch_size, max_len, vocab_size)\n","        # a = 1/0 # stops training after printing everything once\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKVa5ko8x8X6"},"source":["model = LSTM(len(token2idx), 300, 50).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFVm161y09Bq"},"source":["train(\n","    num_epochs=1,\n","    model=model,\n","    train_loader=loaders[\"train\"],\n","    valid_loader=loaders[\"valid\"],\n","    criterion=criterion,\n","    optimizer=optimizer,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAHTd3Kktcc8"},"source":["## Методы генерации текста\n","\n","### Greedy Search\n","\n","$$\n","w_t = \\operatorname{argmax}_{w} P\\left(w \\mid w_{1: t-1}\\right)\n","$$\n","\n","![](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)\n","\n","**Проблема**: Модель быстро начинает повторять одну и ту же фразу. \n","\n","### Beam search\n","\n","![](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)\n","\n","**Проблема**: Модель все еще выдает слишком предсказуемый текст, в отличии от человеческой речи.\n","![](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n","\n","### Sampling\n","\n","$$\n","w_{t} \\sim P\\left(w \\mid w_{1: t-1}\\right)\n","$$\n","\n","![](https://huggingface.co/blog/assets/02_how-to-generate/sampling_search_with_temp.png)\n","\n","**Проблема**: страдает целостность текста. Некоторые фразы получаются слишком случайные.\n","\n","### Top-K Sampling\n","\n","\n","![](https://huggingface.co/blog/assets/02_how-to-generate/top_k_sampling.png)\n","\n","Еще можно использовать top-p sampling. Жадно набирать слова, пока их общая вероятность не станет p. Или можно брать top-10/top-50 слов. \n"]},{"cell_type":"code","metadata":{"id":"jWc4lW_41Ii7"},"source":["prefix = [\"def\", \"train\", \"(\"]\n","encoded_prefix = np.array([token2idx[t] for t in prefix])\n","\n","generated = generate(encoded_prefix, model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkMIZbX92mvd"},"source":["prefix = [\"def\", \"train\", \"(\"]\n","encoded_prefix = np.array([token2idx[t] for t in prefix])\n","\n","\n","for t in np.logspace(0.002, 1, 10):\n","    generated = generate(\n","        encoded_prefix, \n","        model, \n","        receptive_field=20, \n","        length=20,\n","        T=t-1\n","    )\n","    print(f\"Temperature: {t-1}\")\n","    print(\" \".join([idx2token[idx] for idx in generated.cpu().numpy().flatten()]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6LKZuDu-3Tu"},"source":["## References\n","\n","\n","\n","1.   [Заметки из курса ШАДа.](https://lena-voita.github.io/nlp_course/language_modeling.html)\n","2.   [Блогпост по теме генерации текста от huggingface.](https://huggingface.co/blog/how-to-generate) Пока не заморачивайтесь, что там за модель в примере. Мы ее подробно рамерем в одном из следующих занятий.\n","\n"]},{"cell_type":"code","metadata":{"id":"p7lRBwUR2o65"},"source":[""],"execution_count":null,"outputs":[]}]}